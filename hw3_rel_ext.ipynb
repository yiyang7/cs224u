{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Relation extraction using distant supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Bill MacCartney\"\n",
    "__version__ = \"CS224U, Stanford, Spring 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Baseline](#Baseline)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "  1. [Different model factory [1 point]](#Different-model-factory-[1-point])\n",
    "  1. [Directional unigram features [2 points]](#Directional-unigram-features-[2-points])\n",
    "  1. [The part-of-speech tags of the \"middle\" words [2 points]](#The-part-of-speech-tags-of-the-\"middle\"-words-[2-points])\n",
    "  1. [Your original system [4 points]](#Your-original-system-[4-points])\n",
    "1. [Bake-off [1 point]](#Bake-off-[1-point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This homework and associated bake-off are devoted to the developing really effective relation extraction systems using distant supervision. \n",
    "\n",
    "As with the previous assignments, this notebook first establishes a baseline system. The initial homework questions ask you to create additional baselines and suggest areas for innovation, and the final homework question asks you to develop an original system for you to enter into the bake-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](rel_ext_01_task.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rel_ext\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we unite our corpus and KB into a dataset, and create some splits for experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_ext_data_home = os.path.join('data', 'rel_ext_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = rel_ext.Corpus(os.path.join(rel_ext_data_home, 'corpus.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = rel_ext.KB(os.path.join(rel_ext_data_home, 'kb.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rel_ext.Dataset(corpus, kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are not wedded to this set-up for splits. The bake-off will be conducted on a previously unseen test-set, so all of the data in `dataset` is fair game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = dataset.build_splits(\n",
    "    split_names=['tiny', 'train', 'dev'],\n",
    "    split_fracs=[0.01, 0.79, 0.20],\n",
    "    seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tiny': Corpus with 3,474 examples; KB with 445 triples,\n",
       " 'train': Corpus with 263,285 examples; KB with 36,191 triples,\n",
       " 'dev': Corpus with 64,937 examples; KB with 9,248 triples,\n",
       " 'all': Corpus with 331,696 examples; KB with 45,884 triples}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_bag_of_words_featurizer(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[word] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[word] += 1\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizers = [simple_bag_of_words_featurizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = lambda: LogisticRegression(fit_intercept=True, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.841      0.388      0.682        340       5716\n",
      "author                    0.786      0.562      0.728        509       5885\n",
      "capital                   0.621      0.189      0.427         95       5471\n",
      "contains                  0.807      0.585      0.750       3904       9280\n",
      "film_performance          0.792      0.563      0.732        766       6142\n",
      "founders                  0.803      0.387      0.661        380       5756\n",
      "genre                     0.490      0.141      0.328        170       5546\n",
      "has_sibling               0.828      0.251      0.567        499       5875\n",
      "has_spouse                0.874      0.327      0.655        594       5970\n",
      "is_a                      0.703      0.223      0.492        497       5873\n",
      "nationality               0.653      0.156      0.399        301       5677\n",
      "parents                   0.856      0.535      0.765        312       5688\n",
      "place_of_birth            0.681      0.202      0.462        233       5609\n",
      "place_of_death            0.483      0.088      0.255        159       5535\n",
      "profession                0.672      0.182      0.437        247       5623\n",
      "worked_at                 0.795      0.240      0.543        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.730      0.314      0.555       9248      95264\n"
     ]
    }
   ],
   "source": [
    "baseline_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying model weights might yield insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest and lowest feature weights for relation adjoins:\n",
      "\n",
      "     2.485 Córdoba\n",
      "     2.462 Valais\n",
      "     2.429 Taluks\n",
      "     ..... .....\n",
      "    -1.404 century\n",
      "    -1.600 thereby\n",
      "    -2.003 America\n",
      "\n",
      "Highest and lowest feature weights for relation author:\n",
      "\n",
      "     3.152 author\n",
      "     2.683 book\n",
      "     2.599 novel\n",
      "     ..... .....\n",
      "    -2.004 superhero\n",
      "    -2.066 or\n",
      "    -4.572 17th\n",
      "\n",
      "Highest and lowest feature weights for relation capital:\n",
      "\n",
      "     3.361 capital\n",
      "     1.913 city\n",
      "     1.798 km\n",
      "     ..... .....\n",
      "    -1.192 and\n",
      "    -1.302 borough\n",
      "    -1.424 ’\n",
      "\n",
      "Highest and lowest feature weights for relation contains:\n",
      "\n",
      "     2.359 bordered\n",
      "     2.144 districts\n",
      "     2.061 neighborhood\n",
      "     ..... .....\n",
      "    -3.529 6th\n",
      "    -3.837 Ceylon\n",
      "    -6.036 Bronx\n",
      "\n",
      "Highest and lowest feature weights for relation film_performance:\n",
      "\n",
      "     4.147 starring\n",
      "     3.835 co-starring\n",
      "     3.345 alongside\n",
      "     ..... .....\n",
      "    -1.928 Mappillai\n",
      "    -1.936 series\n",
      "    -1.996 Westminster\n",
      "\n",
      "Highest and lowest feature weights for relation founders:\n",
      "\n",
      "     3.922 founder\n",
      "     3.826 founded\n",
      "     2.558 established\n",
      "     ..... .....\n",
      "    -1.782 band\n",
      "    -1.870 William\n",
      "    -1.994 Griffith\n",
      "\n",
      "Highest and lowest feature weights for relation genre:\n",
      "\n",
      "     2.844 series\n",
      "     2.697 game\n",
      "     2.579 album\n",
      "     ..... .....\n",
      "    -1.401 Strokes\n",
      "    -1.410 and\n",
      "    -1.757 at\n",
      "\n",
      "Highest and lowest feature weights for relation has_sibling:\n",
      "\n",
      "     5.237 brother\n",
      "     4.038 sister\n",
      "     2.731 nephew\n",
      "     ..... .....\n",
      "    -1.228 James\n",
      "    -1.335 from\n",
      "    -1.586 Her\n",
      "\n",
      "Highest and lowest feature weights for relation has_spouse:\n",
      "\n",
      "     5.068 wife\n",
      "     4.798 husband\n",
      "     4.471 widow\n",
      "     ..... .....\n",
      "    -1.367 on\n",
      "    -1.375 between\n",
      "    -1.837 44\n",
      "\n",
      "Highest and lowest feature weights for relation is_a:\n",
      "\n",
      "     3.187 family\n",
      "     2.919 \n",
      "     2.432 order\n",
      "     ..... .....\n",
      "    -1.514 while\n",
      "    -1.584 at\n",
      "    -1.617 jack\n",
      "\n",
      "Highest and lowest feature weights for relation nationality:\n",
      "\n",
      "     2.707 born\n",
      "     1.973 ruler\n",
      "     1.926 caliph\n",
      "     ..... .....\n",
      "    -1.345 series\n",
      "    -1.366 and\n",
      "    -1.765 American\n",
      "\n",
      "Highest and lowest feature weights for relation parents:\n",
      "\n",
      "     4.914 son\n",
      "     4.745 father\n",
      "     4.484 daughter\n",
      "     ..... .....\n",
      "    -1.825 away\n",
      "    -1.980 succeeded\n",
      "    -2.163 passes\n",
      "\n",
      "Highest and lowest feature weights for relation place_of_birth:\n",
      "\n",
      "     3.905 born\n",
      "     3.023 birthplace\n",
      "     2.871 mayor\n",
      "     ..... .....\n",
      "    -1.460 American\n",
      "    -1.469 and\n",
      "    -2.097 Oldham\n",
      "\n",
      "Highest and lowest feature weights for relation place_of_death:\n",
      "\n",
      "     2.571 died\n",
      "     2.162 assassinated\n",
      "     1.888 rebuilt\n",
      "     ..... .....\n",
      "    -1.168 that\n",
      "    -1.298 and\n",
      "    -1.812 Westminster\n",
      "\n",
      "Highest and lowest feature weights for relation profession:\n",
      "\n",
      "     3.397 \n",
      "     2.378 American\n",
      "     2.277 British\n",
      "     ..... .....\n",
      "    -1.252 in\n",
      "    -1.395 Texas\n",
      "    -2.170 on\n",
      "\n",
      "Highest and lowest feature weights for relation worked_at:\n",
      "\n",
      "     3.751 professor\n",
      "     2.925 employee\n",
      "     2.924 CEO\n",
      "     ..... .....\n",
      "    -1.171 superhero\n",
      "    -1.201 coast\n",
      "    -1.556 or\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rel_ext.examine_model_weights(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different model factory [1 point]\n",
    "\n",
    "The code in `rel_ext` makes it very easy to experiment with other classifier models: one need only redefine the `model_factory` argument. This question asks you to assess a [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "\n",
    "__To submit:__ A call to `rel_ext.experiment` training on the 'train' part of `splits` and assessing on its `dev` part, with `featurizers` as defined above in this notebook and the `model_factory` set to one based in an `SVC` with `kernel='linear'` and all other arguments left with default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.775      0.365      0.633        340       5716\n",
      "author                    0.725      0.611      0.699        509       5885\n",
      "capital                   0.692      0.284      0.538         95       5471\n",
      "contains                  0.784      0.596      0.738       3904       9280\n",
      "film_performance          0.760      0.624      0.728        766       6142\n",
      "founders                  0.782      0.424      0.669        380       5756\n",
      "genre                     0.551      0.224      0.426        170       5546\n",
      "has_sibling               0.787      0.244      0.545        499       5875\n",
      "has_spouse                0.806      0.350      0.640        594       5970\n",
      "is_a                      0.612      0.264      0.484        497       5873\n",
      "nationality               0.520      0.173      0.371        301       5677\n",
      "parents                   0.832      0.587      0.768        312       5688\n",
      "place_of_birth            0.538      0.215      0.413        233       5609\n",
      "place_of_death            0.486      0.113      0.293        159       5535\n",
      "profession                0.621      0.239      0.470        247       5623\n",
      "worked_at                 0.680      0.289      0.535        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.684      0.350      0.559       9248      95264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import *\n",
    "\n",
    "model_factory = lambda: SVC(kernel='linear')\n",
    "_ = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directional unigram features [2 points]\n",
    "\n",
    "The current bag-of-words representation makes no distinction between \"forward\" and \"reverse\" examples. But, intuitively, there is big difference between _X and his son Y_ and _Y and his son X_. This question asks you to modify `simple_bag_of_words_featurizer` to capture these differences. \n",
    "\n",
    "__To submit:__\n",
    "\n",
    "1. A feature function `directional_bag_of_words_featurizer` that is just like `simple_bag_of_words_featurizer` except that it distinguishes \"forward\" and \"reverse\". To do this, you just need to mark each word feature for whether it is derived from a subject–object example or from an object–subject example. The precise nature of the mark you add for the two cases doesn't make a difference to the model.\n",
    "\n",
    "2. The macro-average F-score on the `dev` set that you obtain from running `rel_ext.experiment` with `directional_bag_of_words_featurizer` as the only featurizer. (Aside from this, use all the default values for `experiment` as exemplified above in this notebook.)\n",
    "\n",
    "3. `rel_ext.experiment` returns some of the core objects used in the experiment. How many feature names does the `vectorizer` have for the experiment run in the previous step? (Note: we're partly asking you to figure out how to get this value by using the sklearn documentation, so please don't ask how to do it on Piazza!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.857      0.424      0.711        340       5716\n",
      "author                    0.832      0.595      0.771        509       5885\n",
      "capital                   0.679      0.200      0.459         95       5471\n",
      "contains                  0.818      0.663      0.782       3904       9280\n",
      "film_performance          0.839      0.658      0.795        766       6142\n",
      "founders                  0.807      0.397      0.669        380       5756\n",
      "genre                     0.759      0.259      0.547        170       5546\n",
      "has_sibling               0.831      0.246      0.564        499       5875\n",
      "has_spouse                0.869      0.345      0.666        594       5970\n",
      "is_a                      0.759      0.241      0.531        497       5873\n",
      "nationality               0.624      0.209      0.447        301       5677\n",
      "parents                   0.885      0.516      0.774        312       5688\n",
      "place_of_birth            0.724      0.236      0.512        233       5609\n",
      "place_of_death            0.667      0.151      0.396        159       5535\n",
      "profession                0.747      0.239      0.524        247       5623\n",
      "worked_at                 0.768      0.260      0.553        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.779      0.353      0.606       9248      95264\n",
      "The macro-average F-score is 0.605\n",
      "number of feature names from the previous steps 40540\n"
     ]
    }
   ],
   "source": [
    "# step 1: define directional_bag_of_words_featurizer\n",
    "def directional_bag_of_words_featurizer(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[(word, \"forward\")] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[(word, \"backward\")] += 1\n",
    "    return feature_counter\n",
    "directional_featurizers = [directional_bag_of_words_featurizer]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# step 2: get the macro-average F score using all default parameters+functions\n",
    "p2_result = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=directional_featurizers,\n",
    "    model_factory=lambda: LogisticRegression(fit_intercept=True, solver='liblinear'),\n",
    "    verbose=True\n",
    ")\n",
    "print(\"The macro-average F-score is\", 0.605)\n",
    "\n",
    "\n",
    "# step 3: # of feature names does the vectorizer have for the experiment run in the previous step\n",
    "print(\"number of feature names from the previous steps\", len(p2_result['vectorizer'].get_feature_names()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The part-of-speech tags of the \"middle\" words [2 points]\n",
    "\n",
    "Our corpus distribution contains part-of-speech (POS) tagged versions of the core text spans. Let's begin to explore whether there is information in these sequences, focusing on `middle_POS`.\n",
    "\n",
    "__To submit:__\n",
    "\n",
    "1. A feature function `middle_bigram_pos_tag_featurizer` that is just like `simple_bag_of_words_featurizer` except that it creates a feature for bigram POS sequences. For example, given \n",
    "\n",
    "  `The/DT dog/N napped/V`\n",
    "  \n",
    "   we obtain the list of bigram POS sequences\n",
    "  \n",
    "   `['<s> DT', 'DT N', 'N V', 'V </s>']`. \n",
    "   \n",
    "   Don't forget the start and end tags, to model those environments properly!\n",
    "\n",
    "2. The macro-average F-score on the `dev` set that you obtain from running `rel_ext.experiment` with `middle_bigram_pos_tag_featurizer` as the only featurizer. (Aside from this, use all the default values for `experiment` as exemplified above in this notebook.)\n",
    "\n",
    "Note: To parse `middle_POS`, one splits on whitespace to get the `word/TAG` pairs. Each of these pairs `s` can be parsed with `s.rsplit('/', 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# helper function\n",
    "def create_bigram(ex):\n",
    "    POS = []\n",
    "    for word in ex.middle_POS.split(' '):\n",
    "        if not word:\n",
    "#             print(\"none\")\n",
    "            continue\n",
    "        term, tag = word.rsplit('/', 1)\n",
    "        POS.append(tag)\n",
    "    # add </s> and <s> at the beginning of the first word and the end of the last word\n",
    "    POS.append('</s>')\n",
    "    POS.insert(0,'<s>')\n",
    "    # convert POS into a bigram:\n",
    "    bigrams = list(nltk.bigrams(POS))    \n",
    "    return bigrams\n",
    "\n",
    "# step 1: define middle_bigram_pos_tag_featurizer\n",
    "def middle_bigram_pos_tag_featurizer(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        # parse the middle_POS and get the \n",
    "        bigrams = create_bigram(ex)\n",
    "        bigram_list = []\n",
    "        for i in bigrams:\n",
    "            i = \" \".join(i)\n",
    "            #bigram_list.append(i)\n",
    "            feature_counter[i] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        # parse the middle_POS and get the \n",
    "        bigrams = create_bigram(ex)\n",
    "        bigram_list = []\n",
    "        for i in bigrams:\n",
    "            i = \" \".join(i)\n",
    "            #bigram_list.append(i)\n",
    "            feature_counter[i] += 1\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.858      0.338      0.656        340       5716\n",
      "author                    0.730      0.334      0.590        509       5885\n",
      "capital                   0.586      0.179      0.403         95       5471\n",
      "contains                  0.757      0.591      0.717       3904       9280\n",
      "film_performance          0.707      0.444      0.632        766       6142\n",
      "founders                  0.560      0.171      0.385        380       5756\n",
      "genre                     0.500      0.182      0.371        170       5546\n",
      "has_sibling               0.724      0.168      0.436        499       5875\n",
      "has_spouse                0.780      0.269      0.566        594       5970\n",
      "is_a                      0.542      0.167      0.374        497       5873\n",
      "nationality               0.377      0.066      0.195        301       5677\n",
      "parents                   0.656      0.256      0.500        312       5688\n",
      "place_of_birth            0.594      0.163      0.389        233       5609\n",
      "place_of_death            0.407      0.069      0.206        159       5535\n",
      "profession                0.657      0.186      0.436        247       5623\n",
      "worked_at                 0.508      0.128      0.319        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.622      0.232      0.448       9248      95264\n"
     ]
    }
   ],
   "source": [
    "# experiment \n",
    "middle_featurizers = [middle_bigram_pos_tag_featurizer]\n",
    "p3_result = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers= middle_featurizers,\n",
    "    model_factory=lambda: LogisticRegression(fit_intercept=True, solver='liblinear'),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-average F-score is: 0.443\n"
     ]
    }
   ],
   "source": [
    "print(\"macro-average F-score is:\", 0.443)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [4 points]\n",
    "\n",
    "There are many options, and this could easily grow into a project. Here are a few ideas:\n",
    "\n",
    "- Try out different classifier models, from `sklearn` and elsewhere.\n",
    "- Add a feature that indicates the length of the middle.\n",
    "- Augment the bag-of-words representation to include bigrams or trigrams (not just unigrams).\n",
    "- Introduce features based on the entity mentions themselves. <!-- \\[SPOILER: it helps a lot, maybe 4% in F-score. And combines nicely with the directional features.\\] -->\n",
    "- Experiment with features based on the context outside (rather than between) the two entity mentions — that is, the words before the first mention, or after the second.\n",
    "- Try adding features which capture syntactic information, such as the dependency-path features used by Mintz et al. 2009. The [NLTK](https://www.nltk.org/) toolkit contains a variety of [parsing algorithms](http://www.nltk.org/api/nltk.parse.html) that may help.\n",
    "- The bag-of-words representation does not permit generalization across word categories such as names of people, places, or companies. Can we do better using word embeddings such as [GloVe](https://nlp.stanford.edu/projects/glove/)?\n",
    "- Consider adding features based on WordNet synsets. Here's a little code to get you started with that:\n",
    "  ```\n",
    "  from nltk.corpus import wordnet as wn\n",
    "  dog_compatible_synsets = wn.synsets('dog', pos='n')\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Try out different classifier models, from sklearn and elsewhere\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "   AdaBoostClassifier(),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.871      0.338      0.662        340       5716\n",
      "author                    0.767      0.361      0.626        509       5885\n",
      "capital                   0.735      0.263      0.541         95       5471\n",
      "contains                  0.799      0.479      0.705       3904       9280\n",
      "film_performance          0.773      0.454      0.678        766       6142\n",
      "founders                  0.931      0.284      0.640        380       5756\n",
      "genre                     0.583      0.082      0.263        170       5546\n",
      "has_sibling               0.799      0.230      0.535        499       5875\n",
      "has_spouse                0.806      0.335      0.629        594       5970\n",
      "is_a                      0.730      0.163      0.430        497       5873\n",
      "nationality               0.840      0.070      0.262        301       5677\n",
      "parents                   0.853      0.519      0.756        312       5688\n",
      "place_of_birth            0.844      0.163      0.460        233       5609\n",
      "place_of_death            0.636      0.044      0.172        159       5535\n",
      "profession                0.672      0.158      0.407        247       5623\n",
      "worked_at                 0.886      0.128      0.406        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.783      0.255      0.511       9248      95264\n"
     ]
    }
   ],
   "source": [
    "# decision tree with max_depth = 5\n",
    "DecisionTreeClassifier_result = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=lambda: DecisionTreeClassifier(max_depth=5),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   1.000      0.035      0.155        340       5716\n",
      "author                    0.000      0.000      0.000        509       5885\n",
      "capital                   0.000      0.000      0.000         95       5471\n",
      "contains                  0.919      0.070      0.267       3904       9280\n",
      "film_performance          0.000      0.000      0.000        766       6142\n",
      "founders                  0.000      0.000      0.000        380       5756\n",
      "genre                     0.000      0.000      0.000        170       5546\n",
      "has_sibling               0.000      0.000      0.000        499       5875\n",
      "has_spouse                0.000      0.000      0.000        594       5970\n",
      "is_a                      0.000      0.000      0.000        497       5873\n",
      "nationality               0.000      0.000      0.000        301       5677\n",
      "parents                   0.000      0.000      0.000        312       5688\n",
      "place_of_birth            0.000      0.000      0.000        233       5609\n",
      "place_of_death            0.000      0.000      0.000        159       5535\n",
      "profession                0.000      0.000      0.000        247       5623\n",
      "worked_at                 0.000      0.000      0.000        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.120      0.007      0.026       9248      95264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yatong_chen/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "RandomForest_result = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=lambda: RandomForestClassifier(max_depth=5, n_estimators=15),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yatong_chen/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/yatong_chen/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    " # MLPClassifier(alpha=1)\n",
    "MLP_result = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=lambda: MLPClassifier(alpha=1),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoostClassifier()\n",
    "AdaBoost_result = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=lambda: AdaBoostClassifier(),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "For the bake-off, we will release a test set right after class on April 29. The announcement will go out on Piazza. You will evaluate your custom model from the previous question on these new datasets using the function `rel_ext.bake_off_experiment`. Rules:\n",
    "\n",
    "1. Only one evaluation is permitted.\n",
    "1. No additional system tuning is permitted once the bake-off has started.\n",
    "\n",
    "To enter the bake-off, upload this notebook on Canvas:\n",
    "\n",
    "https://canvas.stanford.edu/courses/99711/assignments/187248\n",
    "\n",
    "The cells below this one constitute your bake-off entry.\n",
    "\n",
    "People who enter will receive the additional homework point, and people whose systems achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "The bake-off will close at 4:30 pm on May 1. Late entries will be accepted, but they cannot earn the extra 0.5 points. Similarly, you cannot win the bake-off unless your homework is submitted on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your bake-off assessment code in this cell. \n",
    "# Please do not remove this comment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On an otherwise blank line in this cell, please enter\n",
    "# your macro-average f-score (an F_0.5 score) as reported \n",
    "# by the code above. Please enter only a number between \n",
    "# 0 and 1 inclusive. Please do not remove this comment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
