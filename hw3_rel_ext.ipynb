{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Relation extraction using distant supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Bill MacCartney\"\n",
    "__version__ = \"CS224U, Stanford, Spring 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Baseline](#Baseline)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "  1. [Different model factory [1 point]](#Different-model-factory-[1-point])\n",
    "  1. [Directional unigram features [2 points]](#Directional-unigram-features-[2-points])\n",
    "  1. [The part-of-speech tags of the \"middle\" words [2 points]](#The-part-of-speech-tags-of-the-\"middle\"-words-[2-points])\n",
    "  1. [Your original system [4 points]](#Your-original-system-[4-points])\n",
    "1. [Bake-off [1 point]](#Bake-off-[1-point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This homework and associated bake-off are devoted to the developing really effective relation extraction systems using distant supervision. \n",
    "\n",
    "As with the previous assignments, this notebook first establishes a baseline system. The initial homework questions ask you to create additional baselines and suggest areas for innovation, and the final homework question asks you to develop an original system for you to enter into the bake-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](rel_ext_01_task.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rel_ext\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we unite our corpus and KB into a dataset, and create some splits for experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_ext_data_home = os.path.join('data', 'rel_ext_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = rel_ext.Corpus(os.path.join(rel_ext_data_home, 'corpus.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = rel_ext.KB(os.path.join(rel_ext_data_home, 'kb.tsv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rel_ext.Dataset(corpus, kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are not wedded to this set-up for splits. The bake-off will be conducted on a previously unseen test-set, so all of the data in `dataset` is fair game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = dataset.build_splits(\n",
    "    split_names=['tiny', 'train', 'dev'],\n",
    "    split_fracs=[0.01, 0.79, 0.20],\n",
    "    seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tiny': Corpus with 3,474 examples; KB with 445 triples,\n",
       " 'train': Corpus with 263,285 examples; KB with 36,191 triples,\n",
       " 'dev': Corpus with 64,937 examples; KB with 9,248 triples,\n",
       " 'all': Corpus with 331,696 examples; KB with 45,884 triples}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_bag_of_words_featurizer(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[word] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[word] += 1\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizers = [simple_bag_of_words_featurizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = lambda: LogisticRegression(fit_intercept=True, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.850      0.368      0.673        340       5716\n",
      "author                    0.819      0.560      0.750        509       5885\n",
      "capital                   0.500      0.147      0.338         95       5471\n",
      "contains                  0.796      0.596      0.746       3904       9280\n",
      "film_performance          0.808      0.567      0.745        766       6142\n",
      "founders                  0.805      0.403      0.671        380       5756\n",
      "genre                     0.582      0.188      0.410        170       5546\n",
      "has_sibling               0.852      0.242      0.567        499       5875\n",
      "has_spouse                0.865      0.325      0.649        594       5970\n",
      "is_a                      0.714      0.211      0.484        497       5873\n",
      "nationality               0.591      0.173      0.398        301       5677\n",
      "parents                   0.842      0.545      0.759        312       5688\n",
      "place_of_birth            0.603      0.202      0.431        233       5609\n",
      "place_of_death            0.588      0.126      0.339        159       5535\n",
      "profession                0.593      0.194      0.420        247       5623\n",
      "worked_at                 0.709      0.252      0.520        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.720      0.319      0.556       9248      95264\n"
     ]
    }
   ],
   "source": [
    "baseline_results = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying model weights might yield insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest and lowest feature weights for relation adjoins:\n",
      "\n",
      "     2.545 CÃ³rdoba\n",
      "     2.441 Valais\n",
      "     2.416 Taluks\n",
      "     ..... .....\n",
      "    -1.212 Asia\n",
      "    -1.370 thereby\n",
      "    -1.476 Ireland\n",
      "\n",
      "Highest and lowest feature weights for relation author:\n",
      "\n",
      "     3.013 author\n",
      "     2.470 books\n",
      "     2.349 wrote\n",
      "     ..... .....\n",
      "    -2.039 or\n",
      "    -2.053 same\n",
      "    -2.621 infamous\n",
      "\n",
      "Highest and lowest feature weights for relation capital:\n",
      "\n",
      "     2.912 capital\n",
      "     1.827 posted\n",
      "     1.825 especially\n",
      "     ..... .....\n",
      "    -1.189 and\n",
      "    -1.206 largest\n",
      "    -1.410 Westminster\n",
      "\n",
      "Highest and lowest feature weights for relation contains:\n",
      "\n",
      "     2.759 third-largest\n",
      "     2.275 attended\n",
      "     2.222 reached\n",
      "     ..... .....\n",
      "    -3.607 Ceylon\n",
      "    -3.663 Antrim\n",
      "    -6.032 Bronx\n",
      "\n",
      "Highest and lowest feature weights for relation film_performance:\n",
      "\n",
      "     3.931 starring\n",
      "     3.687 co-starring\n",
      "     3.678 opposite\n",
      "     ..... .....\n",
      "    -1.780 comedian\n",
      "    -1.844 or\n",
      "    -1.994 Westminster\n",
      "\n",
      "Highest and lowest feature weights for relation founders:\n",
      "\n",
      "     4.055 founder\n",
      "     3.788 co-founder\n",
      "     3.786 founded\n",
      "     ..... .....\n",
      "    -1.576 city\n",
      "    -1.657 band\n",
      "    -1.688 kingdom\n",
      "\n",
      "Highest and lowest feature weights for relation genre:\n",
      "\n",
      "     3.071 series\n",
      "     2.901 \n",
      "     2.668 game\n",
      "     ..... .....\n",
      "    -1.411 and\n",
      "    -1.423 ;\n",
      "    -1.790 at\n",
      "\n",
      "Highest and lowest feature weights for relation has_sibling:\n",
      "\n",
      "     5.244 brother\n",
      "     3.762 sister\n",
      "     2.910 Marlon\n",
      "     ..... .....\n",
      "    -1.418 Her\n",
      "    -1.522 His\n",
      "    -1.974 formed\n",
      "\n",
      "Highest and lowest feature weights for relation has_spouse:\n",
      "\n",
      "     5.123 wife\n",
      "     4.461 widow\n",
      "     4.245 husband\n",
      "     ..... .....\n",
      "    -1.268 on\n",
      "    -1.360 half\n",
      "    -1.556 Laura\n",
      "\n",
      "Highest and lowest feature weights for relation is_a:\n",
      "\n",
      "     3.381 \n",
      "     2.577 Genus\n",
      "     2.382 family\n",
      "     ..... .....\n",
      "    -1.579 at\n",
      "    -3.174 Talpidae\n",
      "    -5.834 characin\n",
      "\n",
      "Highest and lowest feature weights for relation nationality:\n",
      "\n",
      "     2.673 born\n",
      "     1.913 caliph\n",
      "     1.809 Pinky\n",
      "     ..... .....\n",
      "    -1.503 appointed\n",
      "    -1.635 American\n",
      "    -1.666 2010\n",
      "\n",
      "Highest and lowest feature weights for relation parents:\n",
      "\n",
      "     5.047 son\n",
      "     4.856 daughter\n",
      "     4.209 father\n",
      "     ..... .....\n",
      "    -1.607 via\n",
      "    -1.686 filmmaker\n",
      "    -1.728 half\n",
      "\n",
      "Highest and lowest feature weights for relation place_of_birth:\n",
      "\n",
      "     3.793 born\n",
      "     3.019 birthplace\n",
      "     2.762 mayor\n",
      "     ..... .....\n",
      "    -1.406 or\n",
      "    -1.427 and\n",
      "    -1.840 Westminster\n",
      "\n",
      "Highest and lowest feature weights for relation place_of_death:\n",
      "\n",
      "     2.531 died\n",
      "     2.035 rebuilt\n",
      "     1.835 assassinated\n",
      "     ..... .....\n",
      "    -1.245 and\n",
      "    -1.262 created\n",
      "    -1.993 Westminster\n",
      "\n",
      "Highest and lowest feature weights for relation profession:\n",
      "\n",
      "     3.917 \n",
      "     2.431 philosopher\n",
      "     2.354 American\n",
      "     ..... .....\n",
      "    -1.228 in\n",
      "    -1.258 at\n",
      "    -1.986 on\n",
      "\n",
      "Highest and lowest feature weights for relation worked_at:\n",
      "\n",
      "     3.025 professor\n",
      "     2.997 CEO\n",
      "     2.886 president\n",
      "     ..... .....\n",
      "    -1.224 appointed\n",
      "    -1.379 â\n",
      "    -1.593 or\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rel_ext.examine_model_weights(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different model factory [1 point]\n",
    "\n",
    "The code in `rel_ext` makes it very easy to experiment with other classifier models: one need only redefine the `model_factory` argument. This question asks you to assess a [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "\n",
    "__To submit:__ A call to `rel_ext.experiment` training on the 'train' part of `splits` and assessing on its `dev` part, with `featurizers` as defined above in this notebook and the `model_factory` set to one based in an `SVC` with `kernel='linear'` and all other arguments left with default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.774      0.332      0.611        340       5716\n",
      "author                    0.765      0.613      0.729        509       5885\n",
      "capital                   0.667      0.295      0.532         95       5471\n",
      "contains                  0.784      0.601      0.739       3904       9280\n",
      "film_performance          0.751      0.623      0.721        766       6142\n",
      "founders                  0.740      0.442      0.652        380       5756\n",
      "genre                     0.608      0.282      0.494        170       5546\n",
      "has_sibling               0.793      0.238      0.541        499       5875\n",
      "has_spouse                0.848      0.357      0.665        594       5970\n",
      "is_a                      0.594      0.260      0.473        497       5873\n",
      "nationality               0.467      0.189      0.361        301       5677\n",
      "parents                   0.791      0.593      0.741        312       5688\n",
      "place_of_birth            0.533      0.210      0.408        233       5609\n",
      "place_of_death            0.478      0.138      0.321        159       5535\n",
      "profession                0.504      0.231      0.408        247       5623\n",
      "worked_at                 0.626      0.318      0.525        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.670      0.358      0.558       9248      95264\n"
     ]
    }
   ],
   "source": [
    "model_factory = lambda: SVC(kernel='linear')\n",
    "_ = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=featurizers,\n",
    "    model_factory=model_factory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directional unigram features [2 points]\n",
    "\n",
    "The current bag-of-words representation makes no distinction between \"forward\" and \"reverse\" examples. But, intuitively, there is big difference between _X and his son Y_ and _Y and his son X_. This question asks you to modify `simple_bag_of_words_featurizer` to capture these differences. \n",
    "\n",
    "__To submit:__\n",
    "\n",
    "1. A feature function `directional_bag_of_words_featurizer` that is just like `simple_bag_of_words_featurizer` except that it distinguishes \"forward\" and \"reverse\". To do this, you just need to mark each word feature for whether it is derived from a subjectâobject example or from an objectâsubject example. The precise nature of the mark you add for the two cases doesn't make a difference to the model.\n",
    "\n",
    "2. The macro-average F-score on the `dev` set that you obtain from running `rel_ext.experiment` with `directional_bag_of_words_featurizer` as the only featurizer. (Aside from this, use all the default values for `experiment` as exemplified above in this notebook.)\n",
    "\n",
    "3. `rel_ext.experiment` returns some of the core objects used in the experiment. How many feature names does the `vectorizer` have for the experiment run in the previous step? (Note: we're partly asking you to figure out how to get this value by using the sklearn documentation, so please don't ask how to do it on Piazza!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: define directional_bag_of_words_featurizer\n",
    "def directional_bag_of_words_featurizer(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[(word, \"forward\")] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[(word, \"backward\")] += 1\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_featurizers = [directional_bag_of_words_featurizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = lambda: LogisticRegression(fit_intercept=True, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.880      0.409      0.715        340       5716\n",
      "author                    0.837      0.593      0.773        509       5885\n",
      "capital                   0.649      0.253      0.494         95       5471\n",
      "contains                  0.838      0.655      0.794       3904       9280\n",
      "film_performance          0.877      0.649      0.819        766       6142\n",
      "founders                  0.813      0.413      0.681        380       5756\n",
      "genre                     0.698      0.259      0.521        170       5546\n",
      "has_sibling               0.820      0.246      0.560        499       5875\n",
      "has_spouse                0.872      0.357      0.677        594       5970\n",
      "is_a                      0.734      0.249      0.529        497       5873\n",
      "nationality               0.632      0.223      0.462        301       5677\n",
      "parents                   0.858      0.522      0.760        312       5688\n",
      "place_of_birth            0.691      0.240      0.503        233       5609\n",
      "place_of_death            0.677      0.132      0.371        159       5535\n",
      "profession                0.686      0.239      0.499        247       5623\n",
      "worked_at                 0.835      0.273      0.591        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.775      0.357      0.609       9248      95264\n"
     ]
    }
   ],
   "source": [
    "# step 2: get the macro-average F score using all default parameters+functions\n",
    "p2_result = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=directional_featurizers,\n",
    "    model_factory=model_factory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The macro-average F-score is 0.609\n"
     ]
    }
   ],
   "source": [
    "print(\"The macro-average F-score is\", 0.609)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of feature names from the previous steps 40345\n"
     ]
    }
   ],
   "source": [
    "# step 3: # of feature names does the vectorizer have for the experiment run in the previous step\n",
    "print(\"number of feature names from the previous steps\", len(p2_result['vectorizer'].get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The part-of-speech tags of the \"middle\" words [2 points]\n",
    "\n",
    "Our corpus distribution contains part-of-speech (POS) tagged versions of the core text spans. Let's begin to explore whether there is information in these sequences, focusing on `middle_POS`.\n",
    "\n",
    "__To submit:__\n",
    "\n",
    "1. A feature function `middle_bigram_pos_tag_featurizer` that is just like `simple_bag_of_words_featurizer` except that it creates a feature for bigram POS sequences. For example, given \n",
    "\n",
    "  `The/DT dog/N napped/V`\n",
    "  \n",
    "   we obtain the list of bigram POS sequences\n",
    "  \n",
    "   `['<s> DT', 'DT N', 'N V', 'V </s>']`. \n",
    "   \n",
    "   Don't forget the start and end tags, to model those environments properly!\n",
    "\n",
    "2. The macro-average F-score on the `dev` set that you obtain from running `rel_ext.experiment` with `middle_bigram_pos_tag_featurizer` as the only featurizer. (Aside from this, use all the default values for `experiment` as exemplified above in this notebook.)\n",
    "\n",
    "Note: To parse `middle_POS`, one splits on whitespace to get the `word/TAG` pairs. Each of these pairs `s` can be parsed with `s.rsplit('/', 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def create_bigram(ex):\n",
    "    POS = []\n",
    "    for word in ex.middle_POS.split(' '):\n",
    "        if not word:\n",
    "            continue\n",
    "        term, tag = word.rsplit('/', 1)\n",
    "        POS.append(tag)\n",
    "    # add </s> and <s>\n",
    "    POS.insert(0,'<s>')\n",
    "    POS.append('</s>')\n",
    "    # convert POS into a bigram:\n",
    "    bigrams = list(nltk.bigrams(POS))    \n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: define middle_bigram_pos_tag_featurizer\n",
    "def middle_bigram_pos_tag_featurizer(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        # parse the middle_POS and get the \n",
    "        bigrams = create_bigram(ex)\n",
    "        bigram_list = []\n",
    "        for i in bigrams:\n",
    "            i = \" \".join(i)\n",
    "            #bigram_list.append(i)\n",
    "            feature_counter[i] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        # parse the middle_POS and get the \n",
    "        bigrams = create_bigram(ex)\n",
    "        bigram_list = []\n",
    "        for i in bigrams:\n",
    "            i = \" \".join(i)\n",
    "            #bigram_list.append(i)\n",
    "            feature_counter[i] += 1\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_featurizers = [middle_bigram_pos_tag_featurizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = lambda: LogisticRegression(fit_intercept=True, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.801      0.356      0.641        340       5716\n",
      "author                    0.665      0.352      0.565        509       5885\n",
      "capital                   0.467      0.147      0.326         95       5471\n",
      "contains                  0.758      0.589      0.717       3904       9280\n",
      "film_performance          0.736      0.448      0.652        766       6142\n",
      "founders                  0.550      0.174      0.384        380       5756\n",
      "genre                     0.580      0.171      0.392        170       5546\n",
      "has_sibling               0.672      0.168      0.420        499       5875\n",
      "has_spouse                0.735      0.261      0.539        594       5970\n",
      "is_a                      0.614      0.157      0.388        497       5873\n",
      "nationality               0.431      0.083      0.235        301       5677\n",
      "parents                   0.632      0.276      0.502        312       5688\n",
      "place_of_birth            0.550      0.189      0.398        233       5609\n",
      "place_of_death            0.387      0.075      0.212        159       5535\n",
      "profession                0.639      0.158      0.397        247       5623\n",
      "worked_at                 0.492      0.124      0.309        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.607      0.233      0.442       9248      95264\n"
     ]
    }
   ],
   "source": [
    "p3_result = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers= middle_featurizers,\n",
    "    model_factory=model_factory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-average F-score is: 0.442\n"
     ]
    }
   ],
   "source": [
    "print(\"macro-average F-score is:\", 0.442)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [4 points]\n",
    "\n",
    "There are many options, and this could easily grow into a project. Here are a few ideas:\n",
    "\n",
    "- Try out different classifier models, from `sklearn` and elsewhere.\n",
    "- Add a feature that indicates the length of the middle.\n",
    "- Augment the bag-of-words representation to include bigrams or trigrams (not just unigrams).\n",
    "- Introduce features based on the entity mentions themselves. <!-- \\[SPOILER: it helps a lot, maybe 4% in F-score. And combines nicely with the directional features.\\] -->\n",
    "- Experiment with features based on the context outside (rather than between) the two entity mentions âÂ that is, the words before the first mention, or after the second.\n",
    "- Try adding features which capture syntactic information, such as the dependency-path features used by Mintz et al. 2009. The [NLTK](https://www.nltk.org/) toolkit contains a variety of [parsing algorithms](http://www.nltk.org/api/nltk.parse.html) that may help.\n",
    "- The bag-of-words representation does not permit generalization across word categories such as names of people, places, or companies. Can we do better using word embeddings such as [GloVe](https://nlp.stanford.edu/projects/glove/)?\n",
    "- Consider adding features based on WordNet synsets. Here's a little code to get you started with that:\n",
    "  ```\n",
    "  from nltk.corpus import wordnet as wn\n",
    "  dog_compatible_synsets = wn.synsets('dog', pos='n')\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.909      0.382      0.713        340       5716\n",
      "author                    0.722      0.619      0.699        509       5885\n",
      "capital                   0.846      0.232      0.553         95       5471\n",
      "contains                  0.813      0.418      0.684       3904       9280\n",
      "film_performance          0.877      0.409      0.713        766       6142\n",
      "founders                  0.908      0.284      0.631        380       5756\n",
      "genre                     0.686      0.141      0.387        170       5546\n",
      "has_sibling               0.788      0.230      0.531        499       5875\n",
      "has_spouse                0.815      0.327      0.627        594       5970\n",
      "is_a                      0.811      0.155      0.439        497       5873\n",
      "nationality               0.681      0.163      0.416        301       5677\n",
      "parents                   0.878      0.462      0.744        312       5688\n",
      "place_of_birth            0.782      0.185      0.475        233       5609\n",
      "place_of_death            0.423      0.069      0.209        159       5535\n",
      "profession                0.689      0.170      0.428        247       5623\n",
      "worked_at                 0.784      0.165      0.448        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.776      0.276      0.544       9248      95264\n"
     ]
    }
   ],
   "source": [
    "# directional_featurizers + decision tree with max_depth = 5\n",
    "DecisionTreeClassifier_result = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=directional_featurizers,\n",
    "    model_factory=lambda: DecisionTreeClassifier(max_depth=5),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.825      0.429      0.697        340       5716\n",
      "author                    0.778      0.523      0.709        509       5885\n",
      "capital                   0.690      0.305      0.551         95       5471\n",
      "contains                  0.808      0.566      0.744       3904       9280\n",
      "film_performance          0.843      0.597      0.779        766       6142\n",
      "founders                  0.758      0.363      0.623        380       5756\n",
      "genre                     0.567      0.200      0.415        170       5546\n",
      "has_sibling               0.803      0.212      0.516        499       5875\n",
      "has_spouse                0.805      0.375      0.655        594       5970\n",
      "is_a                      0.740      0.229      0.512        497       5873\n",
      "nationality               0.521      0.203      0.397        301       5677\n",
      "parents                   0.837      0.545      0.756        312       5688\n",
      "place_of_birth            0.611      0.236      0.464        233       5609\n",
      "place_of_death            0.383      0.113      0.259        159       5535\n",
      "profession                0.689      0.251      0.511        247       5623\n",
      "worked_at                 0.728      0.277      0.549        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.712      0.339      0.571       9248      95264\n"
     ]
    }
   ],
   "source": [
    "# directional_featurizers + AdaBoostClassifier()\n",
    "AdaBoost_result = rel_ext.experiment(\n",
    "    splits,\n",
    "    train_split='train',\n",
    "    test_split='dev',\n",
    "    featurizers=directional_featurizers,\n",
    "    model_factory=lambda: AdaBoostClassifier(),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "For the bake-off, we will release a test set right after class on April 29. The announcement will go out on Piazza. You will evaluate your custom model from the previous question on these new datasets using the function `rel_ext.bake_off_experiment`. Rules:\n",
    "\n",
    "1. Only one evaluation is permitted.\n",
    "1. No additional system tuning is permitted once the bake-off has started.\n",
    "\n",
    "To enter the bake-off, upload this notebook on Canvas:\n",
    "\n",
    "https://canvas.stanford.edu/courses/99711/assignments/187248\n",
    "\n",
    "The cells below this one constitute your bake-off entry.\n",
    "\n",
    "People who enter will receive the additional homework point, and people whose systems achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "The bake-off will close at 4:30 pm on May 1. Late entries will be accepted, but they cannot earn the extra 0.5 points. Similarly, you cannot win the bake-off unless your homework is submitted on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your bake-off assessment code in this cell. \n",
    "# Please do not remove this comment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On an otherwise blank line in this cell, please enter\n",
    "# your macro-average f-score (an F_0.5 score) as reported \n",
    "# by the code above. Please enter only a number between \n",
    "# 0 and 1 inclusive. Please do not remove this comment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
